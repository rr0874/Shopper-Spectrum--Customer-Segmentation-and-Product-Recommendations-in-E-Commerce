# -*- coding: utf-8 -*-
"""project_3_Shopper_Spectrumipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L79cIObKvgsVCn6anomowjaVmdDOi-sy

# Project Title---

**Shopper Spectrum: Customer Segmentation and Product Recommendations in E-Commerce**

# Problem Type:

**Unsupervised Machine Learning – Clustering**

**Collaborative Filtering – Recommendation System**

# 1.Dataset Collection and understanding
"""

#importing required liraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#for clusttering
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

#for recommendation system
from sklearn.metrics.pairwise import cosine_similarity

#warnings
import warnings
warnings.filterwarnings('ignore')

print("Libraries Imported Successfully")

# Read the dataset
df = pd.read_csv('/content/drive/MyDrive/3_aug_submission_internship/online_retail.csv', encoding='ISO-8859-1')

# Show the first few rows
df.head()

"""**Missing and Duplicate Values**"""

# Step 1.5: Check for Missing Values
missing_values = df.isnull().sum()
print("Missing Values:\n", missing_values)

# Step 1.6: Check for Duplicates
duplicate_rows = df.duplicated().sum()
print("\nNumber of Duplicate Rows:", duplicate_rows)

# Step: Summary Statistics
df.describe(include='all')

"""#2.Data Preprocessing:

**Remove Rows with Missing CustomerID**
"""

# Step 2.1: Remove rows with missing CustomerID
print("Before removing missing CustomerID:", df.shape)
df = df[~df['CustomerID'].isnull()]
print("After removing missing CustomerID:", df.shape)

"""**Remove Cancelled Invoices
Cancelled invoices start with 'C' in the InvoiceNo column**
"""

# Step 2.2: Remove cancelled invoices (InvoiceNo starting with 'C')
print("Before removing cancelled invoices:", df.shape)
df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]
print("After removing cancelled invoices:", df.shape)

# Step 2.3: Remove negative or zero Quantity and UnitPrice
print("Before removing negative/zero Quantity and Price:", df.shape)
df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]
print("After removing invalid Quantity and Price:", df.shape)

"""**Reset Index After Cleaning**"""

# Step 2.4: Reset index after cleaning
df.reset_index(drop=True, inplace=True)
print("Index reset done. Final cleaned dataset shape:", df.shape)

"""#3.Exploratory Data Analysis (EDA):

**3.1-Analyze Transaction Volume by Country**

*Transaction Volume by Country*
"""

# Step 3.1: Number of transactions per country

country_orders = df.groupby('Country')['InvoiceNo'].nunique().sort_values(ascending=False)

# Display top 10 countries
print("Top 10 Countries by Transaction Volume:\n")
print(country_orders.head(10))

"""*Pie Chart of Top 5 Countries by Transactions*"""

top_5_countries = country_orders.head(5)

plt.figure(figsize=(8,8))
plt.pie(top_5_countries, labels=top_5_countries.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))
plt.title('Top 5 Countries by Transaction Volume')
plt.axis('equal')
plt.show()

"""**3.2 – Identify Top-Selling Products**

*Top Products by Quantity Sold*
"""

#Find top 10 products based on quantity sold
top_products = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)

print("Top 10 Best-Selling Products:\n")
print(top_products)

"""*Bar Plot of Top 10 Products*"""

# Bar plot for top 10 best-selling products
plt.figure(figsize=(12,6))
sns.barplot(x=top_products.values, y=top_products.index, palette='coolwarm')
plt.title('Top 10 Best-Selling Products (by Quantity Sold)')
plt.xlabel('Total Quantity Sold')
plt.ylabel('Product Description')
plt.tight_layout()
plt.show()

"""**3.3-Visualize purchase trends over time**

*Convert InvoiceDate to datetime and Create Daily Sales Data*
"""

# Step 3.3.1: Convert InvoiceDate to datetime format
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Step 3.3.2: Create a new column for just the date (no time)
df['InvoiceDay'] = df['InvoiceDate'].dt.date

# Step 3.3.3: Group by date and sum quantity or transactions
daily_sales = df.groupby('InvoiceDay')['Quantity'].sum()

# Show first few values
daily_sales.head()

"""*Line Plot of Quantity Sold Over Time*"""

# Step 3.3.4: Line plot of daily quantity sold
plt.figure(figsize=(14,6))
daily_sales.plot(kind='line', color='darkblue')
plt.title('Daily Quantity of Products Sold Over Time')
plt.xlabel('Date')
plt.ylabel('Total Quantity Sold')
plt.grid(True)
plt.tight_layout()
plt.show()

"""*to see monthly trends:*"""

# Monthly trend
df['InvoiceMonth'] = df['InvoiceDate'].dt.to_period('M')
monthly_sales = df.groupby('InvoiceMonth')['Quantity'].sum()

# Plot monthly trend
plt.figure(figsize=(12,6))
monthly_sales.plot(marker='o', color='green')
plt.title('Monthly Quantity Sold Trend')
plt.xlabel('Month')
plt.ylabel('Total Quantity Sold')
plt.grid(True)
plt.tight_layout()
plt.show()

"""**3.4-Inspect monetary distribution per transaction and customer**

Inspect monetary distribution per transaction and customer.

We’ll calculate Total Amount (Quantity × UnitPrice) for:

-Each transaction (InvoiceNo)

-Each customer (CustomerID)

And visualize those distributions.

*Create a TotalAmount Column*
"""

# Step 3.4.1: Create TotalAmount column (Quantity * UnitPrice)
df['TotalAmount'] = df['Quantity'] * df['UnitPrice']
df[['InvoiceNo', 'CustomerID', 'TotalAmount']].head()

"""*italicized textDistribution of Total Amount per Transaction*"""

# Step 3.4.2: Total amount per transaction (InvoiceNo)
transaction_amount = df.groupby('InvoiceNo')['TotalAmount'].sum()

plt.figure(figsize=(10,5))
sns.histplot(transaction_amount, bins=100, kde=True, color='blue')
plt.title('Distribution of Total Amount per Transaction')
plt.xlabel('Transaction Amount')
plt.ylabel('Frequency')
plt.xlim(0, 1000)  # Optional: Focus on main range
plt.show()

"""*Distribution of Total Amount per Customer*"""

# Step 3.4.3: Total amount per customer (CustomerID)
customer_amount = df.groupby('CustomerID')['TotalAmount'].sum()

plt.figure(figsize=(10,5))
sns.histplot(customer_amount, bins=100, kde=True, color='orange')
plt.title('Distribution of Total Amount per Customer')
plt.xlabel('Total Spend per Customer')
plt.ylabel('Number of Customers')
plt.xlim(0, 5000)  # Optional: Adjust as needed
plt.show()

"""**3.5-RFM distributions**

RFM Distributions, which is crucial for customer segmentation.

We will calculate and visualize:

R (Recency): Days since last purchase

F (Frequency): Total number of transactions per customer

M (Monetary): Total amount spent per customer

*Calculate RFM Values*
"""

# Step 3.5.1: Reference date for Recency (assume one day after the latest InvoiceDate)
import datetime
reference_date = df['InvoiceDate'].max() + datetime.timedelta(days=1)

# Step 3.5.2: Calculate R, F, M for each CustomerID
rfm = df.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency
    'InvoiceNo': 'nunique',                                    # Frequency
    'TotalAmount': 'sum'                                       # Monetary
})

# Rename columns
rfm.columns = ['Recency', 'Frequency', 'Monetary']
rfm.reset_index(inplace=True)

# Display top rows
rfm.head()

"""*Plot Distributions of R, F, and M*"""

# Step 3.5.3: Plot RFM distributions
plt.figure(figsize=(18,5))

# Recency
plt.subplot(1, 3, 1)
sns.histplot(rfm['Recency'], bins=30, kde=True, color='skyblue')
plt.title('Recency Distribution')
plt.xlabel('Days Since Last Purchase')

# Frequency
plt.subplot(1, 3, 2)
sns.histplot(rfm['Frequency'], bins=30, kde=True, color='orange')
plt.title('Frequency Distribution')
plt.xlabel('Number of Purchases')

# Monetary
plt.subplot(1, 3, 3)
sns.histplot(rfm['Monetary'], bins=30, kde=True, color='green')
plt.title('Monetary Distribution')
plt.xlabel('Total Spend')

plt.tight_layout()
plt.show()

"""**3.6 – Elbow Curve for Optimal k**

*This will help us determine the optimal number of clusters (k) for customer segmentation using the KMeans algorithm on the RFM values.*
"""

# Step 3.6.1: Normalize/Scale the RFM features
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])

print("Scaled RFM data shape:", rfm_scaled.shape)

"""*Elbow Method using Inertia (SSE)*"""

# Step 3.6.2: Elbow Method to find optimal k
sse = []  # Sum of Squared Errors

K = range(1, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(rfm_scaled)
    sse.append(kmeans.inertia_)

# Plotting the Elbow Curve
plt.figure(figsize=(8,5))
plt.plot(K, sse, marker='o')
plt.title('Elbow Method For Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('SSE (Inertia)')
plt.grid(True)
plt.show()

"""*Silhouette Score Analysis (for validation)*"""

# Step 3.6.3: Silhouette Score for each k (2 to 10)
from sklearn.metrics import silhouette_score

silhouette_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(rfm_scaled)
    score = silhouette_score(rfm_scaled, kmeans.labels_)
    silhouette_scores.append(score)

# Plot silhouette scores
plt.figure(figsize=(8,5))
plt.plot(range(2,11), silhouette_scores, marker='o', color='purple')
plt.title('Silhouette Score for different k values')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

"""**Customer cluster profiles**

We’ll now apply KMeans clustering to the scaled RFM data using the optimal k from Step 3.6, assign cluster labels to each customer, and analyze what each cluster represents (like High-Value, At-Risk, etc.).

*Run KMeans with Optimal k*
"""

# Step 3.7.1: Run KMeans with selected number of clusters (e.g., k=4)
k = 10  # update this if needed
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(rfm_scaled)

# Assign cluster labels to original RFM DataFrame
rfm['Cluster'] = kmeans.labels_

# Show sample
rfm.head()

"""
*Cluster Averages (Profile Summary)*

"""

# Step 3.7.2: Analyze cluster characteristics
cluster_summary = rfm.groupby('Cluster').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean',
    'CustomerID': 'count'
}).rename(columns={'CustomerID': 'Count'}).round(2)

cluster_summary

# Step 3.7.3: Visualize cluster profiles
plt.figure(figsize=(12,6))
cluster_summary[['Recency', 'Frequency', 'Monetary']].plot(kind='bar', figsize=(12,6))
plt.title('Customer Segments Profile by Cluster')
plt.ylabel('Average Value')
plt.xlabel('Cluster')
plt.grid(axis='y')
plt.xticks(rotation=0)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

"""Label Clusters (Optional Manual Mapping)
Based on their RFM values, you can assign segment names like:

Cluster Type	Label

Low R, High F & M	                     High-Value

Mid R, F & M	                         Regular

High R, Low F & M	                     At-Risk

Highest R, Lowest F/M	                Occasional

"""

# Step 3.7.4: (Optional) Map segment labels
cluster_labels = {
    0: 'Regular',
    1: 'At-Risk',
    2: 'High-Value',
    3: 'Occasional'
}
rfm['Segment'] = rfm['Cluster'].map(cluster_labels)

rfm[['CustomerID', 'Recency', 'Frequency', 'Monetary', 'Cluster', 'Segment']].head()

"""**3.8-Product recommendation heatmap / similarity matrix**

to set the foundation for the recommendation system.

We'll build a Customer × Product matrix based on purchase frequency, then calculate cosine similarity between products and visualize a similarity heatmap.


"""

# Step 3.8.1: Create pivot table (CustomerID × Product Description)
pivot_df = df.pivot_table(index='CustomerID',
                          columns='Description',
                          values='Quantity',
                          aggfunc='sum',
                          fill_value=0)

pivot_df.head()

"""*Calculate Cosine Similarity Between Products*"""

# Step 3.8.2: Transpose to get Product × Customer matrix
product_customer_matrix = pivot_df.T

# Step 3.8.3: Compute cosine similarity between products
product_similarity = cosine_similarity(product_customer_matrix)
product_similarity_df = pd.DataFrame(product_similarity,
                                     index=product_customer_matrix.index,
                                     columns=product_customer_matrix.index)

product_similarity_df.head()

# Step 3.8.4: Select top 10 most purchased products for heatmap
top_10_products = df['Description'].value_counts().head(10).index
top_10_similarity = product_similarity_df.loc[top_10_products, top_10_products]

# Plot heatmap
plt.figure(figsize=(10,8))
sns.heatmap(top_10_similarity, annot=True, cmap='YlGnBu', fmt=".2f")
plt.title("Product Similarity Heatmap (Top 10 Products)")
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""#4.Clustering Methodology

**Final Cluster Recap Table**

This gives a complete view of each cluster.
"""

# Step 4.1: Final Cluster Summary with Segment Labels
final_cluster_summary = rfm.groupby(['Cluster', 'Segment']).agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean',
    'CustomerID': 'count'
}).rename(columns={'CustomerID': 'CustomerCount'}).round(2)

final_cluster_summary.reset_index()

"""**Save RFM and Model for Streamlit**"""

# Step 4.2: Save rfm data with segments to CSV (for use in Streamlit)
rfm.to_csv('rfm_clustered.csv', index=False)

# Save the scaler and model using joblib
import joblib
joblib.dump(scaler, 'rfm_scaler.pkl')
joblib.dump(kmeans, 'rfm_kmeans_model.pkl')

print("Clustered RFM data and models saved.")

"""**Visualize Clusters in 3D**"""

# Step 4.3: 3D visualization of clusters
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10,7))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(rfm['Recency'], rfm['Frequency'], rfm['Monetary'],
                     c=rfm['Cluster'], cmap='Set2', s=60)

ax.set_xlabel('Recency')
ax.set_ylabel('Frequency')
ax.set_zlabel('Monetary')
ax.set_title('3D View of Customer Clusters')
plt.show()

"""# ⭐5.Recommendation System Approach

**Create Customer × Product Matrix (StockCode version)**

We already did this for descriptions earlier. Now let's make sure we use StockCode as it's more unique.
"""

# Step 5.1: Create pivot table (CustomerID × StockCode)
product_matrix = df.pivot_table(index='CustomerID',
                                 columns='StockCode',
                                 values='Quantity',
                                 aggfunc='sum',
                                 fill_value=0)

product_matrix.head()

"""**Compute Cosine Similarity Between Products**"""

# Step 5.2: Transpose to get StockCode × Customer matrix
product_matrix_T = product_matrix.T

# Compute cosine similarity
product_similarity_matrix = cosine_similarity(product_matrix_T)
product_similarity_df = pd.DataFrame(product_similarity_matrix,
                                     index=product_matrix_T.index,
                                     columns=product_matrix_T.index)

product_similarity_df.head()

"""**Build Recommendation Function (Top 5 Similar Products)**

We’ll also map StockCode → Description for meaningful output.
"""

# Step 5.3: Create a mapping from StockCode to Product Description
product_map = df[['StockCode', 'Description']].drop_duplicates().set_index('StockCode')['Description'].to_dict()

# Step 5.4: Function to get top 5 similar products
def get_similar_products(stock_code, top_n=5):
    if stock_code not in product_similarity_df.columns:
        return f"❌ Product with StockCode '{stock_code}' not found."

    similar_scores = product_similarity_df[stock_code].sort_values(ascending=False)
    similar_codes = similar_scores.index[1:top_n+1]  # exclude itself

    result = []
    for code in similar_codes:
        result.append({
            'StockCode': code,
            'Description': product_map.get(code, 'Unknown'),
            'Similarity': round(similar_scores[code], 3)
        })

    return result

# Example usage
get_similar_products('10080')  # Replace with valid StockCode from your dataset

"""**Save Product Similarity Matrix for Streamlit**"""

# Step 5.5: Save similarity matrix and mapping for Streamlit app
product_similarity_df.to_csv('product_similarity_matrix.csv')
pd.DataFrame.from_dict(product_map, orient='index').reset_index().rename(columns={'index': 'StockCode', 0: 'Description'}).to_csv('product_description_map.csv', index=False)

print("Product similarity matrix and product map saved.")

"""# Cell A – Save Clustering Model and Scaler"""

import joblib

joblib.dump(kmeans, 'rfm_kmeans_model.pkl')
joblib.dump(scaler, 'rfm_scaler.pkl')
print("✅ KMeans model and scaler saved.")

"""#  Save RFM with Clusters to CSV"""

rfm.to_csv('rfm_clustered.csv', index=False)
print("✅ RFM clustered data saved.")

"""# Save Product Similarity Matrix to CSV"""

product_similarity_df.to_csv('product_similarity_matrix.csv')
print("✅ Product similarity matrix saved.")

"""# Save StockCode → Description Mapping to CSV"""

pd.DataFrame.from_dict(product_map, orient='index')\
  .reset_index()\
  .rename(columns={'index': 'StockCode', 0: 'Description'})\
  .to_csv('product_description_map.csv', index=False)

print("✅ Product description map saved.")